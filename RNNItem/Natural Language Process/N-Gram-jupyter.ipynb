{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zhuanlan.zhihu.com/p/27237078\n",
    "# N-Gram language Modeling\n",
    "首先我们介绍一下 N-Gram 模型。在一篇文章中，每一句话有很多单词组成，而对于一句话，这些单词的组成顺序也是很重要的，我们想要知道在一篇文章中我们是否可以给出几个词然后预测这些词后面的一个单词，比如’I lived in France for 10 years, I can speak _ .’那么我们想要做的就是预测最后这个词是French。\n",
    "\n",
    "知道了我们想要做的事情之后，我们就可以引出 N-Gram 模型了。\n",
    "\n",
    "\n",
    "这是一个条件概率，也就是我们给定想要预测的单词的前面几个单词，然后最大化我们想要预测的这个单词的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "首先我们给出了一段文章作为我们的训练集\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTEXT_SIZE表示我们想由前面的几个单词来预测这个单词，这里设置为2，就是说我们希望通过这个单词的前两个单词来预测这一个单词。 EMBEDDING_DIM表示word embedding的维数，上一篇已经介绍过了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = [((test_sentence[i], test_sentence[i+1]), test_sentence[i+2])\n",
    "           for i in range(len(test_sentence)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('When', 'forty'), 'winters'),\n",
       " (('forty', 'winters'), 'shall'),\n",
       " (('winters', 'shall'), 'besiege'),\n",
       " (('shall', 'besiege'), 'thy'),\n",
       " (('besiege', 'thy'), 'brow,')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要将数据整理好，也就是我们需要将单词三个分组，每个组前两个作为传入的数据，而最后一个作为预测的结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocb = set(test_sentence) # 通过set将重复的单词去掉\n",
    "word_to_idx = {word: i for i, word in enumerate(vocb)}\n",
    "idx_to_word = {word_to_idx[word]: word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where 0\n",
      "When 1\n",
      "much 2\n",
      "Proving 3\n",
      "it 4\n",
      "own 5\n"
     ]
    }
   ],
   "source": [
    "for i,(key,value) in enumerate(word_to_idx.items()):\n",
    "    print(key,value)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来需要给每个单词编码，也就是用数字来表示每个单词，这样才能够传入word embeding得到词向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class NgramModel(nn.Module):\n",
    "    def __init__(self, vocb_size, context_size, n_dim):\n",
    "        super(NgramModel, self).__init__()\n",
    "        self.n_word = vocb_size # 传入单词表大小\n",
    "        # 通过nn.Embedding构建随机单词向量，词表大小为n_word,每个向量大小为n_dim,Embedding表为n_word*n_dim矩阵\n",
    "        self.embedding = nn.Embedding(self.n_word, n_dim)\n",
    "        self.linear1 = nn.Linear(context_size*n_dim, 128) # 线性层，输入为context_size*n_dim，输出128个维度\n",
    "        self.linear2 = nn.Linear(128, self.n_word) # 线性层2，输入128个维度，输出n_word个维度，预测相应的下一个词\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(1, -1) # 展平输入的一组向量，展成1维，1*(n_words*n_dim)\n",
    "        out = self.linear1(emb) # 将展平向量输入线性层1\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out) # 将激活后向量输入线性层2\n",
    "        log_prob = F.log_softmax(out) # softmax（激活）\n",
    "        return log_prob\n",
    "\n",
    "ngrammodel = NgramModel(len(word_to_idx), CONTEXT_SIZE, 100)\n",
    "print(ngrammodel.embedding(torch.LongTensor([[1,2,4,5]])).size())\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(ngrammodel.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个模型需要传入的参数是所有的单词数，预测单词需要的前面单词数，即CONTEXT_SIZE，词向量的维度。\n",
    "\n",
    "然后在向前传播中，首先传入单词得到词向量，比如在该模型中传入两个词，得到的词向量是(2, 100)，然后将词向量展开成(1, 200)，然后传入一个线性模型，经过relu激活函数再传入一个线性模型，输出的维数是单词总数，可以看成一个分类问题，要最大化预测单词的概率，最后经过一个log softmax激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snjl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.764704\n",
      "epoch: 2\n",
      "**********\n",
      "Loss: 4.706596\n",
      "epoch: 3\n",
      "**********\n",
      "Loss: 4.648425\n",
      "epoch: 4\n",
      "**********\n",
      "Loss: 4.590196\n",
      "epoch: 5\n",
      "**********\n",
      "Loss: 4.531650\n",
      "epoch: 6\n",
      "**********\n",
      "Loss: 4.472860\n",
      "epoch: 7\n",
      "**********\n",
      "Loss: 4.413735\n",
      "epoch: 8\n",
      "**********\n",
      "Loss: 4.354387\n",
      "epoch: 9\n",
      "**********\n",
      "Loss: 4.294575\n",
      "epoch: 10\n",
      "**********\n",
      "Loss: 4.234422\n",
      "epoch: 11\n",
      "**********\n",
      "Loss: 4.173843\n",
      "epoch: 12\n",
      "**********\n",
      "Loss: 4.112930\n",
      "epoch: 13\n",
      "**********\n",
      "Loss: 4.051579\n",
      "epoch: 14\n",
      "**********\n",
      "Loss: 3.989659\n",
      "epoch: 15\n",
      "**********\n",
      "Loss: 3.927326\n",
      "epoch: 16\n",
      "**********\n",
      "Loss: 3.864514\n",
      "epoch: 17\n",
      "**********\n",
      "Loss: 3.801233\n",
      "epoch: 18\n",
      "**********\n",
      "Loss: 3.737391\n",
      "epoch: 19\n",
      "**********\n",
      "Loss: 3.673175\n",
      "epoch: 20\n",
      "**********\n",
      "Loss: 3.608504\n",
      "epoch: 21\n",
      "**********\n",
      "Loss: 3.543528\n",
      "epoch: 22\n",
      "**********\n",
      "Loss: 3.478260\n",
      "epoch: 23\n",
      "**********\n",
      "Loss: 3.412622\n",
      "epoch: 24\n",
      "**********\n",
      "Loss: 3.346934\n",
      "epoch: 25\n",
      "**********\n",
      "Loss: 3.280687\n",
      "epoch: 26\n",
      "**********\n",
      "Loss: 3.214321\n",
      "epoch: 27\n",
      "**********\n",
      "Loss: 3.147690\n",
      "epoch: 28\n",
      "**********\n",
      "Loss: 3.081027\n",
      "epoch: 29\n",
      "**********\n",
      "Loss: 3.014167\n",
      "epoch: 30\n",
      "**********\n",
      "Loss: 2.947408\n",
      "epoch: 31\n",
      "**********\n",
      "Loss: 2.880593\n",
      "epoch: 32\n",
      "**********\n",
      "Loss: 2.813882\n",
      "epoch: 33\n",
      "**********\n",
      "Loss: 2.747201\n",
      "epoch: 34\n",
      "**********\n",
      "Loss: 2.680463\n",
      "epoch: 35\n",
      "**********\n",
      "Loss: 2.613967\n",
      "epoch: 36\n",
      "**********\n",
      "Loss: 2.547556\n",
      "epoch: 37\n",
      "**********\n",
      "Loss: 2.481298\n",
      "epoch: 38\n",
      "**********\n",
      "Loss: 2.415284\n",
      "epoch: 39\n",
      "**********\n",
      "Loss: 2.349431\n",
      "epoch: 40\n",
      "**********\n",
      "Loss: 2.283941\n",
      "epoch: 41\n",
      "**********\n",
      "Loss: 2.218751\n",
      "epoch: 42\n",
      "**********\n",
      "Loss: 2.154053\n",
      "epoch: 43\n",
      "**********\n",
      "Loss: 2.089823\n",
      "epoch: 44\n",
      "**********\n",
      "Loss: 2.026227\n",
      "epoch: 45\n",
      "**********\n",
      "Loss: 1.963289\n",
      "epoch: 46\n",
      "**********\n",
      "Loss: 1.900972\n",
      "epoch: 47\n",
      "**********\n",
      "Loss: 1.839587\n",
      "epoch: 48\n",
      "**********\n",
      "Loss: 1.778954\n",
      "epoch: 49\n",
      "**********\n",
      "Loss: 1.719396\n",
      "epoch: 50\n",
      "**********\n",
      "Loss: 1.660634\n",
      "epoch: 51\n",
      "**********\n",
      "Loss: 1.603222\n",
      "epoch: 52\n",
      "**********\n",
      "Loss: 1.546763\n",
      "epoch: 53\n",
      "**********\n",
      "Loss: 1.491575\n",
      "epoch: 54\n",
      "**********\n",
      "Loss: 1.437622\n",
      "epoch: 55\n",
      "**********\n",
      "Loss: 1.384957\n",
      "epoch: 56\n",
      "**********\n",
      "Loss: 1.333660\n",
      "epoch: 57\n",
      "**********\n",
      "Loss: 1.283725\n",
      "epoch: 58\n",
      "**********\n",
      "Loss: 1.235223\n",
      "epoch: 59\n",
      "**********\n",
      "Loss: 1.188169\n",
      "epoch: 60\n",
      "**********\n",
      "Loss: 1.142588\n",
      "epoch: 61\n",
      "**********\n",
      "Loss: 1.098414\n",
      "epoch: 62\n",
      "**********\n",
      "Loss: 1.055780\n",
      "epoch: 63\n",
      "**********\n",
      "Loss: 1.014651\n",
      "epoch: 64\n",
      "**********\n",
      "Loss: 0.974911\n",
      "epoch: 65\n",
      "**********\n",
      "Loss: 0.936748\n",
      "epoch: 66\n",
      "**********\n",
      "Loss: 0.899984\n",
      "epoch: 67\n",
      "**********\n",
      "Loss: 0.864724\n",
      "epoch: 68\n",
      "**********\n",
      "Loss: 0.830825\n",
      "epoch: 69\n",
      "**********\n",
      "Loss: 0.798442\n",
      "epoch: 70\n",
      "**********\n",
      "Loss: 0.767334\n",
      "epoch: 71\n",
      "**********\n",
      "Loss: 0.737640\n",
      "epoch: 72\n",
      "**********\n",
      "Loss: 0.709237\n",
      "epoch: 73\n",
      "**********\n",
      "Loss: 0.682154\n",
      "epoch: 74\n",
      "**********\n",
      "Loss: 0.656288\n",
      "epoch: 75\n",
      "**********\n",
      "Loss: 0.631675\n",
      "epoch: 76\n",
      "**********\n",
      "Loss: 0.608171\n",
      "epoch: 77\n",
      "**********\n",
      "Loss: 0.585816\n",
      "epoch: 78\n",
      "**********\n",
      "Loss: 0.564545\n",
      "epoch: 79\n",
      "**********\n",
      "Loss: 0.544332\n",
      "epoch: 80\n",
      "**********\n",
      "Loss: 0.525053\n",
      "epoch: 81\n",
      "**********\n",
      "Loss: 0.506776\n",
      "epoch: 82\n",
      "**********\n",
      "Loss: 0.489424\n",
      "epoch: 83\n",
      "**********\n",
      "Loss: 0.472865\n",
      "epoch: 84\n",
      "**********\n",
      "Loss: 0.457195\n",
      "epoch: 85\n",
      "**********\n",
      "Loss: 0.442279\n",
      "epoch: 86\n",
      "**********\n",
      "Loss: 0.428140\n",
      "epoch: 87\n",
      "**********\n",
      "Loss: 0.414663\n",
      "epoch: 88\n",
      "**********\n",
      "Loss: 0.401883\n",
      "epoch: 89\n",
      "**********\n",
      "Loss: 0.389735\n",
      "epoch: 90\n",
      "**********\n",
      "Loss: 0.378165\n",
      "epoch: 91\n",
      "**********\n",
      "Loss: 0.367163\n",
      "epoch: 92\n",
      "**********\n",
      "Loss: 0.356708\n",
      "epoch: 93\n",
      "**********\n",
      "Loss: 0.346752\n",
      "epoch: 94\n",
      "**********\n",
      "Loss: 0.337272\n",
      "epoch: 95\n",
      "**********\n",
      "Loss: 0.328223\n",
      "epoch: 96\n",
      "**********\n",
      "Loss: 0.319638\n",
      "epoch: 97\n",
      "**********\n",
      "Loss: 0.311409\n",
      "epoch: 98\n",
      "**********\n",
      "Loss: 0.303588\n",
      "epoch: 99\n",
      "**********\n",
      "Loss: 0.296090\n",
      "epoch: 100\n",
      "**********\n",
      "Loss: 0.288947\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('*'*10)\n",
    "    running_loss = 0\n",
    "    for data in trigram:\n",
    "        word, label = data\n",
    "        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "        label = Variable(torch.LongTensor([word_to_idx[label]]))\n",
    "        # forward\n",
    "        out = ngrammodel(word)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.data.item()\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {:.6f}'.format(running_loss / len(word_to_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着进行训练，一共跑100个epoch，在每个epoch中，word代表着预测单词的前面两个词，label表示要预测的词，然后记住需要将他们转换成Variable，接着进入网络得到结果，然后通过loss函数得到loss进行反向传播，更新参数。\n",
    "\n",
    "可以通过预测来检测我们的模型是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snjl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real word is thy, predict word is Thy\n"
     ]
    }
   ],
   "source": [
    "word, label = trigram[3]\n",
    "word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))\n",
    "out = ngrammodel(word)\n",
    "_, predict_label = torch.max(out, 1)\n",
    "predict_word = idx_to_word[predict_label.item()]\n",
    "print('real word is {}, predict word is {}'.format(label, predict_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现我们能够准确地预测这个单词。\n",
    "\n",
    "以上我们介绍了如何通过最简单的单边 N-Gram 模型预测单词，还有一种复杂一点的N-Gram模型通过双边的单词来预测中间的单词，这种模型有个专门的名字，叫 Continuous Bag-of-Words model (CBOW)，具体的内容差别不大，就不再细讲了，代码的实现放在了github上面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
