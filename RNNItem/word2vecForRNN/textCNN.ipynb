{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./logs')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as Data\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = torch.from_numpy(X_train)\n",
    "# y_train = torch.from_numpy(y_train)\n",
    "# X_test = torch.from_numpy(X_test)\n",
    "# y_test = torch.from_numpy(y_test)\n",
    "# x_y_dataset = Data.TensorDataset(X_train, y_train)\n",
    "# test_x_y_dataset = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Tool.tool_for_cut_text as tool_for_cut_text\n",
    "import gensim.models.word2vec as w2v\n",
    "import Tool.similarity_sentence as similarity_sentence\n",
    "\n",
    "model = w2v.Word2Vec.load('C:/shu_item/STEO2/model7/model7.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_info():\n",
    "    cwd = os.getcwd()\n",
    "    real_path = os.getcwd() + '\\experiment1_keywords_type'\n",
    "    os.chdir(real_path)\n",
    "    types = os.listdir()\n",
    "    # 还原路径\n",
    "    os.chdir(cwd)\n",
    "\n",
    "    return [[type, type, get_events_len_by_path(real_path + '\\\\' + type + '\\\\' + type + '.txt'),real_path + '\\\\' + type + '\\\\' + type + '.txt'] for type in types]\n",
    "\n",
    "\n",
    "def get_events_len_by_path(path):\n",
    "    print(path)\n",
    "    events = list()\n",
    "    event = ''\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                event += line\n",
    "            else:\n",
    "                if event != '':\n",
    "                    events.append(event)\n",
    "                    event = ''\n",
    "    return len(events)\n",
    "\n",
    "def get_events_by_path(path,type):\n",
    "    events = list()\n",
    "    event = ''\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                event += line\n",
    "            else:\n",
    "                if event != '':\n",
    "                    events.append([event,type])\n",
    "                    event = ''\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\shu_item\\pytorch-handbook\\RNNItem\\word2vecForRNN\\experiment1_keywords_type\\军事\\军事.txt\n",
      "C:\\shu_item\\pytorch-handbook\\RNNItem\\word2vecForRNN\\experiment1_keywords_type\\娱乐\\娱乐.txt\n",
      "C:\\shu_item\\pytorch-handbook\\RNNItem\\word2vecForRNN\\experiment1_keywords_type\\科技\\科技.txt\n"
     ]
    }
   ],
   "source": [
    "type_info = get_type_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取 事件文本，类别 存入event_data\n",
    "event_data = list()\n",
    "for info in type_info:\n",
    "    event_data.extend(get_events_by_path(info[3],info[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['仅需36分钟 天津将建到北京大兴国际机场高铁\\n2019-02-25 12:33:51\\n今年9月就将通航的北京大兴国际机场，把更广阔的腹地延伸到了京津冀三地。中国铁路设计集团近日（21日）对天津至北京大兴国际机场联络线进行第一次环评公示。记者注意到，未来天津至北京大兴国际机场的联络线跨越京津冀三地，为高速铁路。根据铁路有关部门的规划，这条线路预计今年开工，建成后从天津到北京新机场的时间约36分钟。近日，天津至北京大兴国际机场联络线环境影响评价首次信息公示。今年1月，河北省发改委也发布消息称，新建铁路天津至北京新机场联络线获批，批文显示，新建铁路天津至北京新机场联络线起自天津西站，利用津保铁路至胜芳站，由胜芳站新建线路引出，经安次区、永清县引入京雄城际固安东站，利用京雄城际至北京大兴国际机场。\\n',\n",
       " '科技']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_data[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'军事': 0, '娱乐': 1, '科技': 2}\n"
     ]
    }
   ],
   "source": [
    "# 类别映射为数字\n",
    "def type_encoding(type_info):\n",
    "    type_num_dict = {}\n",
    "    for i,type_name in enumerate(type_info):\n",
    "        type_num_dict[type_name[1]] = i\n",
    "    return type_num_dict\n",
    "\n",
    "type_num_dict = type_encoding(type_info)\n",
    "print(type_num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存入每个事件的时间序列\n",
    "event_series_type = []\n",
    "for event_info in event_data:\n",
    "    event_series = similarity_sentence.get_sentence_words_vector_to_rnn(event_info[0][:200],model)\n",
    "    event_type = type_num_dict[event_info[1]]\n",
    "    event_series_type.append([event_series,event_type])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_series_type[440][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_series = list()\n",
    "events_type = list()\n",
    "for item in event_series_type:\n",
    "    events_series.append(torch.from_numpy(item[0]).float())\n",
    "    events_type.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([163, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_series[88].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('event_series',evnet_series)\n",
    "# np.save('event_type',event_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_series_load = np.load('event_series.npy',allow_pickle=True)\n",
    "# event_type_load = np.load('event_type.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_series_pad = rnn_utils.pad_sequence(events_series,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([580, 172, 100])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_series_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = events_series_pad.numpy()\n",
    "labels = np.array(events_type)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "x_y_dataset = Data.TensorDataset(X_train, y_train)\n",
    "test_x_y_dataset = Data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = Data.DataLoader(dataset=x_y_dataset, batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "test_loader = Data.DataLoader(dataset=test_x_y_dataset, batch_size=BATCH_SIZE,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-19c393dcd2fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RNN' is not defined"
     ]
    }
   ],
   "source": [
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, class_num):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        V = args.embed_num\n",
    "        D = 100\n",
    "        C = class_num\n",
    "        Ci = 1\n",
    "        Co = 128\n",
    "        Ks = [3,4,5]\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (N, Co, W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (N, W, D)\n",
    "        \n",
    "        if self.args.static:\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
    "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
    "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
    "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
    "        '''\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  # (N, C)\n",
    "        return logit\n",
    "\n",
    "n_hidden = 128\n",
    "LR = 0.01\n",
    "\n",
    "\n",
    "rnn = CNN_Text(100, n_hidden, 3)\n",
    "rnn = rnn.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, loss_func, epoch):\n",
    "    model.train()\n",
    "    for step, (data, target) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)  # model output\n",
    "        output = output.to(device)\n",
    "        loss = loss_func(output, target)  # cross entropy loss\n",
    "        optimizer.zero_grad()  # clear gradients for this training step\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients\n",
    "        pred_y = output.max(1, keepdim=True)[1]  # 找到概率最大的下标\n",
    "        correct = pred_y.eq(target.view_as(pred_y)).sum().item()\n",
    "        accuracy = correct / len(target)\n",
    "    \n",
    "    writer.add_scalar(graph_name+'/train accuracy', accuracy, epoch)\n",
    "    writer.add_scalar(graph_name+'/train loss',loss.item(),epoch)\n",
    "\n",
    "\n",
    "def model_test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred_y = output.max(1, keepdim=True)[1]  # 找到概率最大的下标\n",
    "            correct += pred_y.eq(target.view_as(pred_y)).sum().item()\n",
    "\n",
    "    test_len = len(test_loader.dataset)\n",
    "    accuracy = correct / test_len\n",
    "    writer.add_scalar(graph_name +'/test accuracy', accuracy, epoch)\n",
    "\n",
    "    print(\"model_test data accuracy: %.9f\" % accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:101\n",
      "model_test data accuracy: 0.310344828\n",
      "epoch:102\n",
      "model_test data accuracy: 0.301724138\n",
      "epoch:103\n",
      "model_test data accuracy: 0.310344828\n",
      "epoch:104\n",
      "model_test data accuracy: 0.387931034\n",
      "epoch:105\n",
      "model_test data accuracy: 0.465517241\n",
      "epoch:106\n",
      "model_test data accuracy: 0.405172414\n",
      "epoch:107\n",
      "model_test data accuracy: 0.362068966\n",
      "epoch:108\n",
      "model_test data accuracy: 0.456896552\n",
      "epoch:109\n",
      "model_test data accuracy: 0.465517241\n",
      "epoch:110\n",
      "model_test data accuracy: 0.517241379\n",
      "epoch:111\n",
      "model_test data accuracy: 0.482758621\n",
      "epoch:112\n",
      "model_test data accuracy: 0.594827586\n",
      "epoch:113\n",
      "model_test data accuracy: 0.594827586\n",
      "epoch:114\n",
      "model_test data accuracy: 0.637931034\n",
      "epoch:115\n",
      "model_test data accuracy: 0.517241379\n",
      "epoch:116\n",
      "model_test data accuracy: 0.491379310\n",
      "epoch:117\n",
      "model_test data accuracy: 0.698275862\n",
      "epoch:118\n",
      "model_test data accuracy: 0.732758621\n",
      "epoch:119\n",
      "model_test data accuracy: 0.715517241\n",
      "epoch:120\n",
      "model_test data accuracy: 0.750000000\n",
      "epoch:121\n",
      "model_test data accuracy: 0.758620690\n",
      "epoch:122\n",
      "model_test data accuracy: 0.741379310\n",
      "epoch:123\n",
      "model_test data accuracy: 0.810344828\n",
      "epoch:124\n",
      "model_test data accuracy: 0.758620690\n",
      "epoch:125\n",
      "model_test data accuracy: 0.844827586\n",
      "epoch:126\n",
      "model_test data accuracy: 0.827586207\n",
      "epoch:127\n",
      "model_test data accuracy: 0.862068966\n",
      "epoch:128\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:129\n",
      "model_test data accuracy: 0.836206897\n",
      "epoch:130\n",
      "model_test data accuracy: 0.836206897\n",
      "epoch:131\n",
      "model_test data accuracy: 0.844827586\n",
      "epoch:132\n",
      "model_test data accuracy: 0.844827586\n",
      "epoch:133\n",
      "model_test data accuracy: 0.870689655\n",
      "epoch:134\n",
      "model_test data accuracy: 0.870689655\n",
      "epoch:135\n",
      "model_test data accuracy: 0.870689655\n",
      "epoch:136\n",
      "model_test data accuracy: 0.862068966\n",
      "epoch:137\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:138\n",
      "model_test data accuracy: 0.870689655\n",
      "epoch:139\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:140\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:141\n",
      "model_test data accuracy: 0.801724138\n",
      "epoch:142\n",
      "model_test data accuracy: 0.844827586\n",
      "epoch:143\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:144\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:145\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:146\n",
      "model_test data accuracy: 0.870689655\n",
      "epoch:147\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:148\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:149\n",
      "model_test data accuracy: 0.862068966\n",
      "epoch:150\n",
      "model_test data accuracy: 0.870689655\n",
      "epoch:151\n",
      "model_test data accuracy: 0.862068966\n",
      "epoch:152\n",
      "model_test data accuracy: 0.862068966\n",
      "epoch:153\n",
      "model_test data accuracy: 0.862068966\n",
      "epoch:154\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:155\n",
      "model_test data accuracy: 0.862068966\n",
      "epoch:156\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:157\n",
      "model_test data accuracy: 0.870689655\n",
      "epoch:158\n",
      "model_test data accuracy: 0.810344828\n",
      "epoch:159\n",
      "model_test data accuracy: 0.801724138\n",
      "epoch:160\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:161\n",
      "model_test data accuracy: 0.879310345\n",
      "epoch:162\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:163\n",
      "model_test data accuracy: 0.879310345\n",
      "epoch:164\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:165\n",
      "model_test data accuracy: 0.862068966\n",
      "epoch:166\n",
      "model_test data accuracy: 0.870689655\n",
      "epoch:167\n",
      "model_test data accuracy: 0.879310345\n",
      "epoch:168\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:169\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:170\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:171\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:172\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:173\n",
      "model_test data accuracy: 0.905172414\n",
      "epoch:174\n",
      "model_test data accuracy: 0.913793103\n",
      "epoch:175\n",
      "model_test data accuracy: 0.913793103\n",
      "epoch:176\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:177\n",
      "model_test data accuracy: 0.879310345\n",
      "epoch:178\n",
      "model_test data accuracy: 0.905172414\n",
      "epoch:179\n",
      "model_test data accuracy: 0.913793103\n",
      "epoch:180\n",
      "model_test data accuracy: 0.922413793\n",
      "epoch:181\n",
      "model_test data accuracy: 0.922413793\n",
      "epoch:182\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:183\n",
      "model_test data accuracy: 0.931034483\n",
      "epoch:184\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:185\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:186\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:187\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:188\n",
      "model_test data accuracy: 0.879310345\n",
      "epoch:189\n",
      "model_test data accuracy: 0.913793103\n",
      "epoch:190\n",
      "model_test data accuracy: 0.853448276\n",
      "epoch:191\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:192\n",
      "model_test data accuracy: 0.879310345\n",
      "epoch:193\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:194\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:195\n",
      "model_test data accuracy: 0.896551724\n",
      "epoch:196\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:197\n",
      "model_test data accuracy: 0.879310345\n",
      "epoch:198\n",
      "model_test data accuracy: 0.879310345\n",
      "epoch:199\n",
      "model_test data accuracy: 0.887931034\n",
      "epoch:200\n",
      "model_test data accuracy: 0.887931034\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 200  # train the training data n times, to save time, we just train 1 epoch\n",
    "graph_name = '字符200，layer=2，dropout=0.5'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多\n",
    "for epoch in range(101, EPOCH + 1):\n",
    "    print('epoch:{}'.format(epoch))\n",
    "    train(model=rnn, device=DEVICE, train_loader=train_loader, optimizer=optimizer, loss_func=criterion, epoch=epoch)\n",
    "\n",
    "    model_test(model=rnn, device=DEVICE, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
