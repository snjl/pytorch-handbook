{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考 https://github.com/apachecn/pytorch-doc-zh/blob/master/docs/1.0/char_rnn_classification_tutorial.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names\\\\Arabic.txt', 'data/names\\\\Chinese.txt', 'data/names\\\\Czech.txt', 'data/names\\\\Dutch.txt', 'data/names\\\\English.txt', 'data/names\\\\French.txt', 'data/names\\\\German.txt', 'data/names\\\\Greek.txt', 'data/names\\\\Irish.txt', 'data/names\\\\Italian.txt', 'data/names\\\\Japanese.txt', 'data/names\\\\Korean.txt', 'data/names\\\\Polish.txt', 'data/names\\\\Portuguese.txt', 'data/names\\\\Russian.txt', 'data/names\\\\Scottish.txt', 'data/names\\\\Spanish.txt', 'data/names\\\\Vietnamese.txt']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./logs')\n",
    "graph_name = 'dropout0.5-3'\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arabic',\n",
       " 'Chinese',\n",
       " 'Czech',\n",
       " 'Dutch',\n",
       " 'English',\n",
       " 'French',\n",
       " 'German',\n",
       " 'Greek',\n",
       " 'Irish',\n",
       " 'Italian',\n",
       " 'Japanese',\n",
       " 'Korean',\n",
       " 'Polish',\n",
       " 'Portuguese',\n",
       " 'Russian',\n",
       " 'Scottish',\n",
       " 'Spanish',\n",
       " 'Vietnamese']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了category_lines，一个字典变量存储每一种语言及其对应的每一行文本(名字)列表的映射关系。\n",
    "\n",
    "变量all_categories是全部语言种类的列表，\n",
    "\n",
    "变量n_categories 是语言种类的数量，后续会使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单词转化为张量\n",
    "现在我们已经加载了所有的名字，我们需要将它们转换为张量来使用它们。\n",
    "\n",
    "我们使用大小为<1 x n_letters>的“one-hot 向量”表示一个字母。\n",
    "\n",
    "一个one-hot向量所有位置都填充为0，并在其表示的字母的位置表示为1，例如\"b\" = <0 1 0 0 0 ...>.（字母b的编号是2，第二个位置是1，其他位置是0）\n",
    "\n",
    "我们使用一个<line_length x 1 x n_letters>的2D矩阵表示一个单词\n",
    "\n",
    "额外的1维是batch的维度，PyTorch默认所有的数据都是成batch处理的。我们这里只设置了batch的大小为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 从所有的字母中得到某个letter的索引编号, 例如 \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "print(lineToTensor('abcdefg').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineToTensor('abcdefg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造神经网络\n",
    "在autograd之前，要在Torch中构建一个可以复制之前时刻层参数的循环神经网络。\n",
    "\n",
    "layer的隐藏状态和梯度将交给计算图自己处理。\n",
    "\n",
    "这意味着你可以像实现的常规的 feed-forward 层一样，以很纯粹的方式实现RNN。\n",
    "\n",
    "这个RNN组件 (几乎是从这里复制的 the PyTorch for Torch users tutorial) 仅使用两层 linear 层对输入和隐藏层做处理,\n",
    "\n",
    "在最后添加一层 LogSoftmax 层预测最终输出。\n",
    "\n",
    "nn.LogSoftmax作为最后一层layer时，nn.NLLLoss作为损失函数是合适的。\n",
    "\n",
    "也可以直接使用rnn的单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,         # rnn hidden unit\n",
    "            num_layers=2,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "            dropout=0.5,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, 64)\n",
    "        self.out2 = nn.Linear(64,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        out = self.out2(out)\n",
    "        return out\n",
    "\n",
    "n_hidden = 128\n",
    "LR = 0.01\n",
    "\n",
    "\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "rnn = rnn.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "## 训练前的准备\n",
    "进行训练步骤之前我们需要构建一些辅助函数。\n",
    "\n",
    "第一个是当我们知道输出结果对应每种类别的可能性时，解析神经网络的输出。\n",
    "\n",
    "我们可以使用 Tensor.topk函数得到最大值在结果中的位置索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还需要一种快速获取训练示例（得到一个名字及其所属的语言类别）的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = list()\n",
    "index = 0\n",
    "train = list()\n",
    "for category in all_categories:\n",
    "    for name in category_lines[category]:\n",
    "        data = lineToTensor(name)\n",
    "        data = data.reshape(-1,57)\n",
    "        train.append(data)\n",
    "        category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "        labels.append(category_tensor)\n",
    "        index += 1    \n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考https://zhuanlan.zhihu.com/p/59772104，使用pad_sequence填充0\n",
    "train = rnn_utils.pad_sequence(train, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape\n",
    "train = train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "x_y_dataset = Data.TensorDataset(X_train, y_train)\n",
    "test_x_y_dataset = Data.TensorDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = Data.DataLoader(dataset=x_y_dataset, batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "test_loader = Data.DataLoader(dataset=test_x_y_dataset, batch_size=BATCH_SIZE,shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, device, train_loader, optimizer, loss_func, epoch):\n",
    "    model.train()\n",
    "    for step, (data, target) in enumerate(train_loader):  # gives batch data, normalize x when iterate train_loader\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)  # model output\n",
    "        output = output.to(device)\n",
    "        loss = loss_func(output, target)  # cross entropy loss\n",
    "        optimizer.zero_grad()  # clear gradients for this training step\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients\n",
    "        pred_y = output.max(1, keepdim=True)[1]  # 找到概率最大的下标\n",
    "        correct = pred_y.eq(target.view_as(pred_y)).sum().item()\n",
    "        accuracy = correct / len(target)\n",
    "    \n",
    "    writer.add_scalar(graph_name+'/train accuracy', accuracy, epoch)\n",
    "    writer.add_scalar(graph_name+'/train loss',loss.item(),epoch)\n",
    "\n",
    "\n",
    "def model_test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred_y = output.max(1, keepdim=True)[1]  # 找到概率最大的下标\n",
    "            correct += pred_y.eq(target.view_as(pred_y)).sum().item()\n",
    "\n",
    "    test_len = len(test_loader.dataset)\n",
    "    accuracy = correct / test_len\n",
    "    writer.add_scalar(graph_name +'/test accuracy', accuracy, epoch)\n",
    "\n",
    "    print(\"model_test dat accuracy: %.9f\" % accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础训练方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n",
      "model_test dat accuracy: 0.472229141\n",
      "epoch:2\n",
      "model_test dat accuracy: 0.590784558\n",
      "epoch:3\n",
      "model_test dat accuracy: 0.597509340\n",
      "epoch:4\n",
      "model_test dat accuracy: 0.643088418\n",
      "epoch:5\n",
      "model_test dat accuracy: 0.693399751\n",
      "epoch:6\n",
      "model_test dat accuracy: 0.704856787\n",
      "epoch:7\n",
      "model_test dat accuracy: 0.744956413\n",
      "epoch:8\n",
      "model_test dat accuracy: 0.764881694\n",
      "epoch:9\n",
      "model_test dat accuracy: 0.780821918\n",
      "epoch:10\n",
      "model_test dat accuracy: 0.785554172\n",
      "epoch:11\n",
      "model_test dat accuracy: 0.790286426\n",
      "epoch:12\n",
      "model_test dat accuracy: 0.796513076\n",
      "epoch:13\n",
      "model_test dat accuracy: 0.803486924\n",
      "epoch:14\n",
      "model_test dat accuracy: 0.794271482\n",
      "epoch:15\n",
      "model_test dat accuracy: 0.806973848\n",
      "epoch:16\n",
      "model_test dat accuracy: 0.809215442\n",
      "epoch:17\n",
      "model_test dat accuracy: 0.806226650\n",
      "epoch:18\n",
      "model_test dat accuracy: 0.812204234\n",
      "epoch:19\n",
      "model_test dat accuracy: 0.813449564\n",
      "epoch:20\n",
      "model_test dat accuracy: 0.810211706\n",
      "epoch:21\n",
      "model_test dat accuracy: 0.811955168\n",
      "epoch:22\n",
      "model_test dat accuracy: 0.807222914\n",
      "epoch:23\n",
      "model_test dat accuracy: 0.814196762\n",
      "epoch:24\n",
      "model_test dat accuracy: 0.801245330\n",
      "epoch:25\n",
      "model_test dat accuracy: 0.805728518\n",
      "epoch:26\n",
      "model_test dat accuracy: 0.817683686\n",
      "epoch:27\n",
      "model_test dat accuracy: 0.814445828\n",
      "epoch:28\n",
      "model_test dat accuracy: 0.807222914\n",
      "epoch:29\n",
      "model_test dat accuracy: 0.811457036\n",
      "epoch:30\n",
      "model_test dat accuracy: 0.800000000\n",
      "epoch:31\n",
      "model_test dat accuracy: 0.811207970\n",
      "epoch:32\n",
      "model_test dat accuracy: 0.813947696\n",
      "epoch:33\n",
      "model_test dat accuracy: 0.814445828\n",
      "epoch:34\n",
      "model_test dat accuracy: 0.800747198\n",
      "epoch:35\n",
      "model_test dat accuracy: 0.810958904\n",
      "epoch:36\n",
      "model_test dat accuracy: 0.810958904\n",
      "epoch:37\n",
      "model_test dat accuracy: 0.810958904\n",
      "epoch:38\n",
      "model_test dat accuracy: 0.812951432\n",
      "epoch:39\n",
      "model_test dat accuracy: 0.805230386\n",
      "epoch:40\n",
      "model_test dat accuracy: 0.812453300\n",
      "epoch:41\n",
      "model_test dat accuracy: 0.812453300\n",
      "epoch:42\n",
      "model_test dat accuracy: 0.812702366\n",
      "epoch:43\n",
      "model_test dat accuracy: 0.814445828\n",
      "epoch:44\n",
      "model_test dat accuracy: 0.801992528\n",
      "epoch:45\n",
      "model_test dat accuracy: 0.804483188\n",
      "epoch:46\n",
      "model_test dat accuracy: 0.795765878\n",
      "epoch:47\n",
      "model_test dat accuracy: 0.809215442\n",
      "epoch:48\n",
      "model_test dat accuracy: 0.805977584\n",
      "epoch:49\n",
      "model_test dat accuracy: 0.810460772\n",
      "epoch:50\n",
      "model_test dat accuracy: 0.808468244\n",
      "epoch:51\n",
      "model_test dat accuracy: 0.808966376\n",
      "epoch:52\n",
      "model_test dat accuracy: 0.808219178\n",
      "epoch:53\n",
      "model_test dat accuracy: 0.808966376\n",
      "epoch:54\n",
      "model_test dat accuracy: 0.802490660\n",
      "epoch:55\n",
      "model_test dat accuracy: 0.800249066\n",
      "epoch:56\n",
      "model_test dat accuracy: 0.812204234\n",
      "epoch:57\n",
      "model_test dat accuracy: 0.807222914\n",
      "epoch:58\n",
      "model_test dat accuracy: 0.803237858\n",
      "epoch:59\n",
      "model_test dat accuracy: 0.796264010\n",
      "epoch:60\n",
      "model_test dat accuracy: 0.811457036\n",
      "epoch:61\n",
      "model_test dat accuracy: 0.803985056\n",
      "epoch:62\n",
      "model_test dat accuracy: 0.810460772\n",
      "epoch:63\n",
      "model_test dat accuracy: 0.811457036\n",
      "epoch:64\n",
      "model_test dat accuracy: 0.811457036\n",
      "epoch:65\n",
      "model_test dat accuracy: 0.806475716\n",
      "epoch:66\n",
      "model_test dat accuracy: 0.804732254\n",
      "epoch:67\n",
      "model_test dat accuracy: 0.815691158\n",
      "epoch:68\n",
      "model_test dat accuracy: 0.816189290\n",
      "epoch:69\n",
      "model_test dat accuracy: 0.804234122\n",
      "epoch:70\n",
      "model_test dat accuracy: 0.810460772\n",
      "epoch:71\n",
      "model_test dat accuracy: 0.802988792\n",
      "epoch:72\n",
      "model_test dat accuracy: 0.806973848\n",
      "epoch:73\n",
      "model_test dat accuracy: 0.805977584\n",
      "epoch:74\n",
      "model_test dat accuracy: 0.809962640\n",
      "epoch:75\n",
      "model_test dat accuracy: 0.801245330\n",
      "epoch:76\n",
      "model_test dat accuracy: 0.811706102\n",
      "epoch:77\n",
      "model_test dat accuracy: 0.803735990\n",
      "epoch:78\n",
      "model_test dat accuracy: 0.807222914\n",
      "epoch:79\n",
      "model_test dat accuracy: 0.806475716\n",
      "epoch:80\n",
      "model_test dat accuracy: 0.805728518\n",
      "epoch:81\n",
      "model_test dat accuracy: 0.806475716\n",
      "epoch:82\n",
      "model_test dat accuracy: 0.802739726\n",
      "epoch:83\n",
      "model_test dat accuracy: 0.805230386\n",
      "epoch:84\n",
      "model_test dat accuracy: 0.810460772\n",
      "epoch:85\n",
      "model_test dat accuracy: 0.813947696\n",
      "epoch:86\n",
      "model_test dat accuracy: 0.810709838\n",
      "epoch:87\n",
      "model_test dat accuracy: 0.807721046\n",
      "epoch:88\n",
      "model_test dat accuracy: 0.807970112\n",
      "epoch:89\n",
      "model_test dat accuracy: 0.813698630\n",
      "epoch:90\n",
      "model_test dat accuracy: 0.810958904\n",
      "epoch:91\n",
      "model_test dat accuracy: 0.813947696\n",
      "epoch:92\n",
      "model_test dat accuracy: 0.812453300\n",
      "epoch:93\n",
      "model_test dat accuracy: 0.805230386\n",
      "epoch:94\n",
      "model_test dat accuracy: 0.800000000\n",
      "epoch:95\n",
      "model_test dat accuracy: 0.804483188\n",
      "epoch:96\n",
      "model_test dat accuracy: 0.801743462\n",
      "epoch:97\n",
      "model_test dat accuracy: 0.794022416\n",
      "epoch:98\n",
      "model_test dat accuracy: 0.803486924\n",
      "epoch:99\n",
      "model_test dat accuracy: 0.799750934\n",
      "epoch:100\n",
      "model_test dat accuracy: 0.800249066\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 100  # train the training data n times, to save time, we just train 1 epoch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    print('epoch:{}'.format(epoch))\n",
    "    train(model=rnn, device=DEVICE, train_loader=train_loader, optimizer=optimizer, loss_func=criterion, epoch=epoch)\n",
    "\n",
    "    model_test(model=rnn, device=DEVICE, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
